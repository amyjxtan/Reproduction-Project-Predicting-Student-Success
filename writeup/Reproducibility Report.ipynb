{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5888d8",
   "metadata": {},
   "source": [
    "# Reproducibility of Enhancement of E-Learning Student’s Performance Based on Ensemble Techniques by Abdulkream A. Alsulami, Abdullah S. AL-Malaise AL-Ghamdi, & Mahmoud Ragab (2023, Electronics)\n",
    "\n",
    "\n",
    "Author: Amy Tan (amyjxtan@gmail.com)\n",
    "\n",
    "Date: `r format(Sys.time(), '%B %d, %Y')`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The original paper, [\"Enhancement of E-Learning Student’s Performance Based on Ensemble Techniques\"](https://github.com/ucsd-psych201a/alsulami2023/blob/main/original_paper/Enhancement_of_ELearning_Students_Performance_Based_on_Ensemble_Techniques.pdf), aimed to improve educational data mining, or EDM, specifically in regards to electronic learning (e-learning) since the COVID-19 pandemic saw a surge in E-learning programs. EDM involves developing methods to deal with the different types of data in educational systems to improve students’ learning outcomes. In particular, the researchers sought to predict student performance using decision trees, naive Bayes, and random forests, enhancing the accuracy further through bagging and boosting. Researchers ultimately concluded that most accurate model methods used decision trees, coupled with boosting, resulting in an accuracy of 0.77.\n",
    "\n",
    "In my project, I sought to reproduce their findings and their visualizations in Python. I anticipated the largest challenges to be implementing the various EDM techniques as I wasn't extremely well-versed in either of them. However, I was confident that the challenge would not prove too difficult, and would be an extremely valuable learning experience. I chose this particular paper to reproduce since it fit into my niche of interests. The work I hope to do in the future involves an intersection of education, cognitive science, and computational tools - this paper provided the opportunity to further my knowledge and skills in how computational tools can and are used to better our education system. My work is available in my [project repository](https://github.com/ucsd-psych201a/alsulami2023/tree/main).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88725180",
   "metadata": {},
   "source": [
    "## Design Overview \n",
    "\n",
    "The paper's design did not include any manipulation of variables, but the data set contains 17 columns (measures) for 480 students. The gathering of the dataset and the following models that were built using it did not involve a within-participants or between-particiapnts design - measures were not repeated. If the dataset had been aimed at examining, for example, a specific learning method's effects on academic success, it could have been done either between or within subjects. The authors did not mention anything about reducing demand characteristics and since the paper isn't specifically aimed at determining a casual relationship, there aren't any confounds. I argue that more information in the original paper could be given on how the data was collected and why they chose to measure the specific variables they did. Without additional information, we are forced to take the dataset at face value and it's difficult to critically examine the data set design choices. There are limits to the models and their generalizability - since the data they are using are do not include folks from Western countries, it's difficult to say that this model would be accurate when applied to a different data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec564070",
   "metadata": {},
   "source": [
    "## Methods\n",
    "### Materials\n",
    "\n",
    "#### The Data/Sample\n",
    "\n",
    "The dataset used in this project (provided in the data folder in `xAPI-Edu-Data.csv`, and also available on [Kaggle](https://www.kaggle.com/datasets/aljarah/xAPI-Edu-Data)) is the exact one used in the original paper. It was \"obtained from the Kalboard 360 E-Learning system via the Experience API (XAPI). The data set in this study consists of 480 records with 17 attributes.\" All attributes are either integer or categorical and are generally categorized into three major attribute types: demographic, academic, and behavioral.\n",
    "\n",
    "### Procedure\n",
    "\n",
    "\"First, we collect the data set and prepare it to perform the study. Then, three traditional data mining methods will apply (decision tree (DT), naïve Bayes (NB), and random forest (RF)) to produce a performance model. In addition to the classifiers mentioned earlier, two ensemble methods are used to improve their performance. Boosting, as well as bagging, is applied to enhance the student prediction model’s success. Two and three methods were added to each ensemble technique using the voting process for a more accurate prediction. The model’s last phase will involve evaluating and discussing the results. The data were divided into training and test sets. Each prediction model’s performance was evaluated using K-fold cross-validation. When testing a model, this technique is used to solve the variance problem. In brief, k-fold cross-validation divides the training set into 10 folds. During training, 9 folds are applied before the final fold is tested. As an average of the different accuracies is taken, this better represents the model performance. The method was repeated ten times. All models were run with the WEKA software’s default parameters.\" My analysis differs in that the models will be built and run in Python.\n",
    "\n",
    "### Analysis Plan\n",
    "\n",
    "#### Data Cleaning\n",
    "\n",
    "\"As part of preprocessing, data cleaning is essential for removing irrelevant objects and missing values in the data collection. There are zero missing values in the data set.\" Though the authors do not specifically mention this, I cleaned the data set with specific regard to the `NationalITy`, `PlaceofBirth`, and `StageID` columns to ensure that column contents were consistent in their usage of capitalization and abbreviations (or lack thereof). I did not exclude any of the data from the provided data set.\n",
    "\n",
    "#### Features Selection\n",
    "\n",
    "\"Feature selection refers to selecting the relevant features of a dataset based on specific criteria from an original feature set. There are two types of data reduction methods: wrapper methods and filter methods. The filter method ranks the features using variable ranking methods, with the highly ranked features being selected and implemented into the learning algorithm. In this study, the information gain ranking filter and a correlation- ranking filter were used. At each decision tree node, and in order to select the test attribute, the information gain measure is taken into account. The information gain (IG) metric determines features with a large number of values. It is calculated with Equation (1).\n",
    "\n",
    "$$IG(T, a) = H(T) − H(T|a)   (1)$$\n",
    "\n",
    "where $T$ is a random variable and $H(T|a)$ is the entropy of $T$ given the value of attribute a.\n",
    "\n",
    "Correlation coefficients are applied to measure correlations among attributes and classes and inter-correlations between features. It is calculated with Equation (2).\n",
    "\n",
    "$$\\rho(X,Y) = \\frac{\\text{cov}(X,Y)}{\\sigma_X\\sigma_Y} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   $X$ and $Y$ are the two variables being correlated\n",
    "-   $n$ is the number of data points\n",
    "-   $x_i$ and $y_i$ are the values of $X$ and $Y$ for the data point\n",
    "-   $\\bar{x}$ and $\\bar{y}$ are the means of $X$ and $Y$\n",
    "-   $\\text{cov}(X,Y)$ is the covariance between $X$ and $Y$\"\n",
    "\n",
    "#### Data Mining Tool and Model Creation\n",
    "\n",
    "After the most relevant features were selected, I used these to create a decision tree (DT), naïve Bayes (NB), and random forest (RF) classifiers. However, as previously mentioned, my project will not use WEKA, but Python. I anticipated using [skit-learn](https://scikit-learn.org/stable/) to accomplish building and testing these models. Following that, I applied boosting and bagging to all the models to test potential improvements.\"\n",
    "\n",
    "#### Measurement Measures\n",
    "\n",
    "\"Different DM techniques were compared to determine which had higher prediction accuracy than others, and a decision was made based on that. The following common metrics can evaluate a study’s performance: accuracy, precision, recall, and F-Measure.\" Thus, my project also examined all four measurements of accuracy to see if I could reproduce a similar finding that **decision trees with boosting had the highest accuracy of 0.77**. The following includes how the authors calculated each of the following, for which I followed suit.\n",
    "\n",
    "##### Accuracy\n",
    "\n",
    "\"This represents the classifier’s accuracy and relates to the classifier’s capacity. The accuracy of a predictor relates to the way it accurately predicts the impact of a predicted fea- ture for new information. The percentage of correct predictions divided by the total number of predictions yields the accuracy. It is calculated with the following Equation (3):\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "where: \n",
    "- True positives ($TP$): cases that are predicted as yes. \n",
    "- True negatives ($TN$): cases that are predicted as no. \n",
    "- False positives ($FP$): cases that are predicted yes and are actually yes. \n",
    "- False negatives ($FN$): cases that are predicted as no but are actually yes.\n",
    "\n",
    "##### Precision\n",
    "\n",
    "Precision is calculated as the ratio of correctly classified positive predictions to total positive predictions, whether correctly or incorrectly classified. It is calculated with Equation (4).\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "##### Recall\n",
    "\n",
    "The recall is determined by calculating the proportion of correctly classified positive predictions to all positive predictions. It is calculated with Equation (5).\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "##### F-Measure\n",
    "\n",
    "F-measure conveys both recall and precision in a single measure. It is calculated with Equation (6).\"\n",
    "\n",
    "$$ F1 − measure = (2 ∗ Recall ∗ Precision)/(Recall + Precision)$$\n",
    "\n",
    "### Differences from Original Study\n",
    "\n",
    "Again, a key difference between this reproducibility project and the original paper are the computational tools used. The original paper used WEKA to visualize and perform machine learning/ensemble methods, whereas my project will use Python. Current anticipated packages include [seaborn](https://seaborn.pydata.org/index.html) and [skit-learn](https://scikit-learn.org/stable/). Additionally, I plan to clean the data using [pandas](https://pandas.pydata.org/) to ensure that entries are consistent, but I do not anticipate any changes I made to make a difference in the findings.\n",
    "\n",
    "### Methods Addendum (Post Data Collection)\n",
    "\n",
    "You can comment this section out prior to final report with data collection.\n",
    "\n",
    "#### Actual Sample\n",
    "\n",
    "Sample size, demographics, data exclusions based on rules spelled out in analysis plan\n",
    "\n",
    "#### Differences from pre-data collection methods plan\n",
    "\n",
    "Any differences from what was described as the original plan, or “none”.\n",
    "\n",
    "## Results\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "Data preparation following the analysis plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad2bc704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Placeofbirth</th>\n",
       "      <th>Stageid</th>\n",
       "      <th>Gradeid</th>\n",
       "      <th>Sectionid</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Semester</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Raisedhands</th>\n",
       "      <th>Visitedresources</th>\n",
       "      <th>Announcementsview</th>\n",
       "      <th>Discussion</th>\n",
       "      <th>Parentansweringsurvey</th>\n",
       "      <th>Parentschoolsatisfaction</th>\n",
       "      <th>Studentabsencedays</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>lowerlevel</td>\n",
       "      <td>G-04</td>\n",
       "      <td>A</td>\n",
       "      <td>IT</td>\n",
       "      <td>F</td>\n",
       "      <td>Father</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Good</td>\n",
       "      <td>Under-7</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>lowerlevel</td>\n",
       "      <td>G-04</td>\n",
       "      <td>A</td>\n",
       "      <td>IT</td>\n",
       "      <td>F</td>\n",
       "      <td>Father</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Good</td>\n",
       "      <td>Under-7</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>lowerlevel</td>\n",
       "      <td>G-04</td>\n",
       "      <td>A</td>\n",
       "      <td>IT</td>\n",
       "      <td>F</td>\n",
       "      <td>Father</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>No</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Above-7</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>lowerlevel</td>\n",
       "      <td>G-04</td>\n",
       "      <td>A</td>\n",
       "      <td>IT</td>\n",
       "      <td>F</td>\n",
       "      <td>Father</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>No</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Above-7</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>lowerlevel</td>\n",
       "      <td>G-04</td>\n",
       "      <td>A</td>\n",
       "      <td>IT</td>\n",
       "      <td>F</td>\n",
       "      <td>Father</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>No</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Above-7</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender Nationality Placeofbirth     Stageid Gradeid Sectionid Topic  \\\n",
       "0      M      Kuwait       Kuwait  lowerlevel    G-04         A    IT   \n",
       "1      M      Kuwait       Kuwait  lowerlevel    G-04         A    IT   \n",
       "2      M      Kuwait       Kuwait  lowerlevel    G-04         A    IT   \n",
       "3      M      Kuwait       Kuwait  lowerlevel    G-04         A    IT   \n",
       "4      M      Kuwait       Kuwait  lowerlevel    G-04         A    IT   \n",
       "\n",
       "  Semester Relation  Raisedhands  Visitedresources  Announcementsview  \\\n",
       "0        F   Father           15                16                  2   \n",
       "1        F   Father           20                20                  3   \n",
       "2        F   Father           10                 7                  0   \n",
       "3        F   Father           30                25                  5   \n",
       "4        F   Father           40                50                 12   \n",
       "\n",
       "   Discussion Parentansweringsurvey Parentschoolsatisfaction  \\\n",
       "0          20                   Yes                     Good   \n",
       "1          25                   Yes                     Good   \n",
       "2          30                    No                      Bad   \n",
       "3          35                    No                      Bad   \n",
       "4          50                    No                      Bad   \n",
       "\n",
       "  Studentabsencedays Class  \n",
       "0            Under-7     M  \n",
       "1            Under-7     M  \n",
       "2            Above-7     L  \n",
       "3            Above-7     L  \n",
       "4            Above-7     M  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Load Relevant Libraries and Functions\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#### Import data\n",
    "\n",
    "edu = pd.read_csv('../data/xAPI-Edu-Data.csv')\n",
    "\n",
    "#### Data exclusion / filtering - NA\n",
    "\n",
    "#### Prepare data for analysis - create columns etc.\n",
    "\n",
    "# Clean dataset column names \n",
    "edu.columns = edu.columns.str.title()\n",
    "\n",
    "# Clean Placeofbirth column to clean random capitalization\n",
    "edu[\"Placeofbirth\"] = edu[\"Placeofbirth\"].str.title()\n",
    "\n",
    "# Clean Stageid column to clean random capitalization\n",
    "edu[\"Stageid\"] = edu[\"Stageid\"].str.lower()\n",
    "\n",
    "# Create a function to turn KW abbrevations into Kuwait for entry consistency in Nationality column\n",
    "def kuwait (string):\n",
    "    if string == 'KW':\n",
    "        return 'Kuwait'\n",
    "    else:\n",
    "        return string \n",
    "    \n",
    "# Apply function kuwait to Nationality column\n",
    "edu['Nationality'] = edu['Nationality'].apply(kuwait)\n",
    "\n",
    "# Double check that all cleaning was correct:\n",
    "edu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a320a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c005ceb5",
   "metadata": {},
   "source": [
    "\n",
    "### Confirmatory analysis\n",
    "\n",
    "The analyses as specified in the analysis plan.\n",
    "\n",
    "*Side-by-side graph with original graph is ideal here*\n",
    "\n",
    "### Exploratory analyses\n",
    "\n",
    "Any follow-up analyses desired (not required).\n",
    "\n",
    "## Discussion\n",
    "\n",
    "### Summary of Replication Attempt\n",
    "\n",
    "Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.\n",
    "\n",
    "### Commentary\n",
    "\n",
    "Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt. None of these need to be long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a84d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
